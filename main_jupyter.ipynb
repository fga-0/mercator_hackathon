{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sb\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "path_OSSE_train = \"../data/OSSE_U_V_SLA_SST_train.nc\"\n",
    "path_OSSE_test = \"../data/OSSE_U_V_SLA_SST_test.nc\"\n",
    "path_eddies_train = \"../data/eddies_train.nc\"\n",
    "\n",
    "eddies_train = xr.open_dataset(path_eddies_train)\n",
    "OSSE_train = xr.open_dataset(path_OSSE_train)\n",
    "OSSE_test = xr.open_dataset(path_OSSE_test)\n",
    "\n",
    "varnames = [\"vozocrtxT\", \"vomecrtyT\", \"sossheig\", \"votemper\"]\n",
    "label_var = [\"eddies\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replace NaN values corresponding to lands by 999."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variables = list(OSSE_train.data_vars.keys())\n",
    "print(\"There are {} variables : {}\".format(len(variables), variables))\n",
    "nan_mask = (\n",
    "    OSSE_train[variables[0]].isnull() &\n",
    "    OSSE_train[variables[1]].isnull() &\n",
    "    OSSE_train[variables[2]].isnull() &\n",
    "    OSSE_train[variables[3]].isnull()\n",
    ")\n",
    "OSSE_train_cleaned = OSSE_train.where(~nan_mask, 999)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, p = 0.5):\n",
    "        super(EncoderBlock, self).__init__()\n",
    "\n",
    "        # Convolution layer n°1\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1)\n",
    "\n",
    "        # Batch normalization n°1\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "        # Convolution layer n°2\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1)\n",
    "\n",
    "        # Batch normalization n°2\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "        # ReLU activation\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "        # AlphaDropout layer\n",
    "        self.alpha_dropout = nn.AlphaDropout(p=p)\n",
    "\n",
    "        # Max pooling layer for downsampling\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        # Apply the first convolution + BatchNorm + ReLU activation\n",
    "        x = self.relu(self.bn1(self.conv1(x)))\n",
    "        # Apply the second convolution + BatchNorm + ReLU activation\n",
    "        x = self.relu(self.bn2(self.conv2(x)))\n",
    "        # Save output for skip connection\n",
    "        skip_connection = x\n",
    "        # Apply AlphaDropout before pooling\n",
    "        x = self.alpha_dropout(x)\n",
    "        # Apply max pooling to reduce spatial dimensions\n",
    "        next_layer = self.pool(x)\n",
    "        return next_layer, skip_connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BottleneckBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, p=0.5):\n",
    "        super(BottleneckBlock, self).__init__()\n",
    "\n",
    "        # Convolution layer n°1\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1)\n",
    "        \n",
    "        # Batch normalization n°1\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        \n",
    "        # Convolution layer n°2\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1)\n",
    "        \n",
    "        # Batch normalization n°2\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "        # AlphaDropout Layer\n",
    "        self.alpha_dropout = nn.AlphaDropout(p=p)\n",
    "\n",
    "        # ReLU activation\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        # Apply convolutions + BatchNorm + ReLU to the input tensor\n",
    "        x = self.relu(self.conv1(x))\n",
    "        x = self.relu(self.conv2(x))\n",
    "        x = self.alpha_dropout(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, p=0.5):\n",
    "        super(DecoderBlock, self).__init__()\n",
    "\n",
    "        # Transposed convolution for upsampling\n",
    "        self.upconv = nn.ConvTranspose2d(in_channels, out_channels, kernel_size=2, stride=2)\n",
    "        # Convolution layers for feature refinement\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        self.alpha_dropout = nn.AlphaDropout(p=p)\n",
    "        # ReLU activation\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x, skip_connection):\n",
    "        \n",
    "        # Upsample the input tensor using transposed convolution\n",
    "        x = self.upconv(x)\n",
    "        # Concatenate the upsampled tensor with the corresponding encoder skip connection\n",
    "        x = torch.cat([x, skip_connection], dim=1)\n",
    "        # Apply convolutions to refine the features\n",
    "        x = self.relu(self.bn1(self.conv1(x)))\n",
    "        x = self.alpha_dropout(x)\n",
    "        x = self.relu(self.bn2(self.conv2(x)))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNet(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, init_features=32, p = 0.2):\n",
    "        super(UNet, self).__init__()\n",
    "        features = init_features\n",
    "\n",
    "        # Encoder Path\n",
    "        self.encoder1 = EncoderBlock(in_channels, features, p=p)\n",
    "        self.encoder2 = EncoderBlock(features, features * 2, p=p)\n",
    "        self.encoder3 = EncoderBlock(features * 2, features * 4, p=p)\n",
    "        self.encoder4 = EncoderBlock(features * 4, features * 8, p=p)\n",
    "\n",
    "        # Bottleneck\n",
    "        self.bottleneck = BottleneckBlock(features * 8, features * 16, p=p)\n",
    "\n",
    "        # Decoder Path\n",
    "        self.decoder4 = DecoderBlock(features * 16, features * 8, p=p)\n",
    "        self.decoder3 = DecoderBlock(features * 8, features * 4, p=p)\n",
    "        self.decoder2 = DecoderBlock(features * 4, features * 2, p=p)\n",
    "        self.decoder1 = DecoderBlock(features * 2, features, p=p)\n",
    "        \n",
    "        # Output Layer\n",
    "        self.output_conv = nn.Conv2d(features, out_channels, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        # Encoder Path\n",
    "        x, skip1 = self.encoder1(x)\n",
    "        x, skip2 = self.encoder2(x)\n",
    "        x, skip3 = self.encoder3(x)\n",
    "        x, skip4 = self.encoder4(x)\n",
    "\n",
    "        # Bottleneck\n",
    "        x = self.bottleneck(x)\n",
    "\n",
    "        # Decoder Path\n",
    "        x = self.decoder4(x, skip4)\n",
    "        x = self.decoder3(x, skip3)\n",
    "        x = self.decoder2(x, skip2)\n",
    "        x = self.decoder1(x, skip1)\n",
    "\n",
    "        # Output Layer\n",
    "        output = self.output_conv(x)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader=None,\n",
    "                num_epochs=5, learning_rate =1e-3, device):\n",
    "    \"\"\"\n",
    "    Training function\n",
    "    \n",
    "    Args:\n",
    "        model: PyTorch model\n",
    "        train_loader: Training data loader\n",
    "        val_loader: Validation data loader\n",
    "        num_epochs: Number of training epochs\n",
    "        learning_rate\n",
    "        device: Device to train on\n",
    "    \"\"\"\n",
    "    model = model.to(device)\n",
    "\n",
    "    # Loss and Optimizer\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index = -1)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode='min', factor=0.1, patience=5)\n",
    "    \n",
    "    # Training Loop\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        total_train_batches = len(train_loader)\n",
    "\n",
    "        for batch_idx, (images, masks) in enumerate(train_loader):\n",
    "            images, masks = images.to(device), masks.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, masks)\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "\n",
    "            # Print every batch update\n",
    "            processed_samples = (batch_idx + 1) * images.size(0)\n",
    "            total_samples = len(train_loader.dataset)\n",
    "            percent_complete = 100.0 * (batch_idx + 1) / total_train_batches\n",
    "            print(\n",
    "                \"Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\".format(\n",
    "                    epoch + 1,\n",
    "                    processed_samples,\n",
    "                    total_samples,\n",
    "                    percent_complete,\n",
    "                    loss.item(),\n",
    "                )\n",
    "            )\n",
    "\n",
    "        # Average training loss\n",
    "        train_loss /= total_train_batches\n",
    "        \n",
    "        # Optional Validation\n",
    "        if val_loader:\n",
    "            model.eval()\n",
    "            val_loss = 0.0\n",
    "            total_val_batches = len(val_loader)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                for batch_idx, (images, masks) in enumerate(val_loader):\n",
    "                    images, masks = images.to(device), masks.to(device)\n",
    "                    outputs = model(images)\n",
    "                    loss = criterion(outputs, masks)\n",
    "                    val_loss += loss.item()\n",
    "\n",
    "                    val_log_str = \"Validation: Batch {}/{} \\tLoss: {:.6f}\".format(\n",
    "                        batch_idx + 1, total_val_batches, loss.item()\n",
    "                    )\n",
    "                \n",
    "                print(val_log_str)\n",
    "\n",
    "            val_loss /= total_val_batches\n",
    "            scheduler.step(val_loss)\n",
    "        \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OceanDataset(Dataset):\n",
    "    def __init__(self, X, Y):\n",
    "        \"\"\"\n",
    "        X : np.ndarray, shape (T, 4, H, W)\n",
    "        Y : np.ndarray, shape (T, H, W)\n",
    "        \"\"\"\n",
    "        self.X = X\n",
    "        self.Y = Y\n",
    "    def __len__(self):\n",
    "        return self.X.shape[0]  # T\n",
    "    def __getitem__(self, idx):\n",
    "        x_arr = self.X[idx]  # shape (4, H, W)\n",
    "        y_arr = self.Y[idx]  # shape (H, W)\n",
    "        # Convert to torch.Tensor\n",
    "        x_torch = torch.from_numpy(x_arr).float()\n",
    "        y_torch = torch.from_numpy(y_arr).long()  # pour CrossEntropyLoss\n",
    "        \n",
    "        return x_torch, y_torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stacked.shape = (284, 4, 357, 717)\n",
      "Lez go Training\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "LAT = 357\n",
    "LONG = 717\n",
    "IMG_SIZE = (LAT, LONG)\n",
    "BATCH_SIZE = 32\n",
    "LEARNING_RATE = 1e-3\n",
    "EPOCHS = 1\n",
    "\n",
    "labels_da = eddies_train[\"eddies\"]\n",
    "Y = labels_da.values\n",
    "# print(\"Data type of Y:\", Y.dtype)\n",
    "# print(Y)\n",
    "Y[np.isnan(Y)] = -1\n",
    "\n",
    "phys_arrays = []\n",
    "for v in varnames:\n",
    "    da_v = OSSE_train[v]\n",
    "    phys_arrays.append(da_v.values)\n",
    "X = np.stack(phys_arrays, axis=1)\n",
    "print(\"stacked.shape =\", X.shape)\n",
    "\n",
    "model = UNet(in_channels=len(varnames), out_channels=3)\n",
    "\n",
    "T = X.shape[0]\n",
    "train_size = int(0.8 * T)\n",
    "val_size = T - train_size\n",
    "train_X, val_X = X[:train_size], X[train_size:]\n",
    "train_Y, val_Y = Y[:train_size], Y[train_size:]\n",
    "train_dataset = OceanDataset(train_X, train_Y)\n",
    "val_dataset = OceanDataset(val_X, val_Y)\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# Train Model\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"Lez go Training\")\n",
    "trained_model = train_model(\n",
    "    model, \n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    num_epochs=EPOCHS, \n",
    "    learning_rate=LEARNING_RATE,\n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sdd_deeplearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
